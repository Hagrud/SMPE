---
title: "SMPE"
author: "PHILIBERT Manon, POLET Pierre-Etienne, SINTIARI Ni Luh Dewi"
date: "December 1, 2016"
output: html_document
---


```{r}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

#1 : First look at the dataset.
We take the data provided by the teacher and plot it to have an overview.
```{r}
data = read.csv("archive_quicksort/_measurements.csv")
plot(data)
head(data)
```

Next we organise it to see the time of execution of each algorithm related to the size of the sorted list.
```{r}
ggplot(data, aes(x=Size, y=Time, color = factor(Type))) + geom_point()
```

We try to analyse this data by te linear regression. But as we can see, this data does not provide much information, so we don't get the result as expected.
```{r}
#test = aggregate(dataSeq$Time, by=list(Category=dataSeq$Size), FUN=mean)
dataSeq = data[data$Type==" Sequential",]
reg <- lm(data = dataSeq, dataSeq$Time~I(dataSeq$Size*log(dataSeq$Size)))
par(mfrow = c(2,2))
plot(reg)
summary(reg)
```
We need more data to make the conclusion. So we do some new experiments, and here is the result.

#2 : Making new data.
We run the code provided in the archive to get more data, and add new size of sorted lists.
```{r}
data_1 = read.csv("archive_quicksort/measurements_computer1.csv")

ggplot(data_1, aes(x=Size, y=Time, color = factor(Type))) + geom_point()
```

##2.1 : Data Analysis.
On this experiment, we see that the parallel algorithm have the bigger variance than the others. We can observe that comparing to the first experiment, for the large n, parallel algorithm runs much faster than the two others. But in this experiment, it runs very slow comparing to the two others. 

```{r}
dataSeq_1 = data_1[data$Type==" Sequential",]
reg <- lm(data = dataSeq_1, dataSeq_1$Time~I(dataSeq_1$Size*log(dataSeq_1$Size)))
summary(reg)
```

```{r}

graph = ggplot(dataSeq_1, aes(x=Size, y=Time, color = factor(Type))) + geom_point()

graph + geom_smooth(method = "lm", formula = y ~ I(x*log(x)))
```

For the sequential algorithm, we obtain that the best regression line is y = O(xlog(x)) 
(so far, we don't know if there is another one which is better).


```{r}
dataBIn_1 = data_1[data$Type==" Built-in",]
reg <- lm(data = dataBIn_1, dataBIn_1$Time~I(dataBIn_1$Size*log(dataBIn_1$Size)))
summary(reg)
graph = ggplot(dataBIn_1, aes(x=Size, y=Time, color = factor(Type))) + geom_point()

graph + geom_smooth(method = "lm", formula = y ~ I(x*log(x)), color ="black")
```

The built-in method give almost the same results that the sequential one, can we assume that they can be implemented in the same way?


```{r}
dataPar_1 = data_1[data$Type==" Parallel",]
reg <- lm(data = dataPar_1, dataPar_1$Time~I(dataPar_1$Size*log(dataPar_1$Size)))
summary(reg)
graph = ggplot(dataPar_1, aes(x=Size, y=Time, color = factor(Type))) + geom_point(color="green")

graph + geom_smooth(method = "lm", formula = y ~ I(x*log(x)), color="black")
```

This regression line is y = O(x*log(x)), but it doesn't match with the data. So, we try to approximate with y = O(log(x)), and the result is the following.

```{r}
reg <- lm(data = dataPar_1, dataPar_1$Time~I(log(dataPar_1$Size)))
summary(reg)
graph = ggplot(dataPar_1, aes(x=Size, y=Time, color = factor(Type))) + geom_point(color="green")

graph + geom_smooth(method = "lm", formula = y ~ log(x), color="black")
```

This one is better than the previous one.

## 2.2 : Need more data ?
We see that the parallel algorithm have a totally different result between the two dataset, we decide to do other experiments with more computers.


```{r}
my_read_data = function(filename) {
  df = read.csv(filename);
  df$origin = filename
  df
}
data = my_read_data("archive_quicksort/_measurements.csv")
data_1 = my_read_data("archive_quicksort/measurements_computer1.csv")
data_2 = my_read_data("archive_quicksort/measurements_computer2.csv")
data_3 = my_read_data("archive_quicksort/measurements_computer3.csv")


data_tot=rbind(data, data_1, data_2,data_3)

data_tot$origin = gsub(pattern = ".*_",replacement = "",data_tot$origin)
data_tot$origin = gsub(pattern = ".csv",replacement = "",data_tot$origin)
data_tot$origin = gsub(pattern = "measurements",replacement = "origin",data_tot$origin)


#ggplot(data_tot, aes(x=Size, y=Time, color = factor(Type))) + ggtitle("initial data set") + geom_point() + facet_wrap(~origin)
ggplot(data, aes(x=Size, y=Time, color = factor(Type))) + ggtitle("initial data set") + geom_point()
ggplot(data_1, aes(x=Size, y=Time, color = factor(Type))) + ggtitle("computer 1") + geom_point()
ggplot(data_2, aes(x=Size, y=Time, color = factor(Type))) + ggtitle("computer 2") + geom_point()
ggplot(data_3, aes(x=Size, y=Time, color = factor(Type))) + ggtitle("computer 3") + geom_point()
```

We can observe that all computers show that for small n, Built-in algorithm gives the better result compared to the Sequential one. But as n grows, the Sequential algorithm provides the better executing time. 
Now, let's observe the parallel algorithm. In the first two computers we can see that up to n = 1000000, the result of parallel algorithm stay above the two others, but in the third computers it intersects when n < 1000000. It seems that it because of the different architecture of the computers. 

In fact, we cannot say which algorithm is the best one, because the result depends on the value of n. In general, we think that the parallel algorithm gives the best result when n is sufficiently large.
```{r}

data_1_2 = my_read_data("archive_quicksort/measurements_computer1_2.csv")
ggplot(data_1_2, aes(x=Size, y=Time, color = factor(Type))) + ggtitle("computer 1") + geom_point()
```

We take more data on computer1 to check if the parallel algorithm give better result on large n.

```{r}
#rmarkdown::render("smpe.rmd")
```

```{r, include=FALSE}
#ggplot(data, aes(x=Size, y=Time, color = factor(Type))) + coord_trans(x = "log2", y = "log2") + #geom_point()
```

```{r,  include=FALSE}
#dataSeq = data[data$Type==" Sequential",]
#ggplot(dataSeq, aes(x=Size, y=Time))  + coord_trans(x = "log2", y = "log2") + geom_point()
```